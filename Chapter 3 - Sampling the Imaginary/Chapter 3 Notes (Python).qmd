---
title: "Untitled"
format: html
---


```{python}
import polars as pl
import pymc as pm
import matplotlib.pyplot as plt
import numpy as np
import arviz as az
from scipy import stats
from toolz import frequencies, valmap
```

```{python}
plt.style.use("bmh")
```

```{python}
Pr_Positive_Vampire = 0.95
Pr_Positive_Mortal = 0.01
Pr_Vampire = 0.001
Pr_Positive = Pr_Positive_Vampire * Pr_Vampire + Pr_Positive_Mortal * (1 - Pr_Vampire)
Pr_Vampire_Positive = Pr_Positive_Vampire * Pr_Vampire / Pr_Positive

Pr_Vampire_Positive
```

## 3.1 Sampling from a grid-approximate posterior
```{python}
p_grid = np.linspace(start=0, stop=1, num=1000)
prob_p = np.repeat(1, 1000)
prob_data = stats.binom(n=9, p=p_grid).pmf(k = 6)
posterior = prob_data * prob_p
posterior = posterior / np.sum(posterior)
```

We can use our posterior distribution to sample values of $p$. Here, each value of $p$ will appear in proportion to the posterior plausibility of each value.

```{python}
rng = np.random.default_rng()
samples = rng.choice(a=p_grid, size=10_000, replace=True, p=posterior)
```

```{python}
fig, ax = plt.subplots()
ax.scatter(np.arange(0, len(samples)), samples)
```

```{python}
fig, ax = plt.subplots()
ax.hist(samples, bins=30)
```

## 3.2 Sampling to summarize

We can use our posterior distribution to answer questions like:

* How much posterior probability lies below a parameter value?
* How much posterior probability lies between two parameter values?
* Which parameter value marks the lower 5% of the posterior probability?
* Which range of parameter values contains 90% of the posterior probability?
* Which parameter value has the highest posterior probability?

### 3.2.1 Intervals of defined boundaries

We won't be able to do below since grid approximation isn't usually practical (especially with multiple parameters). However, we can use the samples we pulled above and do the same thing.
```{python}
np.sum(posterior[p_grid < 0.5])
```

Here, we do what we did above but using samples of $p$:

16.81% posterior probability lies below $p = 0.5$.
```{python}
np.sum(samples < 0.5) / 10_000
```

```{python}
np.mean(samples < 0.5)
```

61.52% posterior probability lies between $p = 0.5 \text{ and } p = 0.75$
```{python}
np.sum((samples > 0.5) & (samples < 0.75)) / 10_000
```

```{python}
np.mean((samples > 0.5) & (samples < 0.75))
```

### 3.2.2 Intervals of defined mass

We can also get how much posterior probability is between different hypothetical values of $p$.

For instance, we see that 80% of the posterior probability lies between 0 and 0.75:


```{python}
np.quantile(samples, 0.8)
```

While the middle 80% is between 0.445 and 0.812
```{python}
np.quantile(samples, [0.1, 0.9])
```

However, this method of finding the posterior probability between two values of $p$ is not as accurate when the distribution is asymmetric. 
```{python}
p_grid = np.linspace(start=0, stop=1, num=1000)
prior = np.repeat(1, 1000)
likelihood = stats.binom(n=9, p=p_grid).pmf(k=6)
posterior = likelihood * prior
posterior = posterior / np.sum(posterior)
samples = rng.choice(a=p_grid, size=10_000, replace=True, p=posterior)
```

In those cases, we can use the **Highest Density Interval (HDI)**, which is the narrowest interval containing a certain amount of the posterior prob
```{python}
az.hdi(samples, hdi_prob=0.5)
```

```{python}
p_grid[np.argmax(posterior)]
```

```{python}
np.mean(samples)
```

```{python}
np.median(samples)
```

```{python}
np.sum(posterior * np.abs(0.5 - p_grid))
```

```{python}
loss = [np.sum(posterior * np.abs(d - p_grid))
            for d in p_grid]
```

```{python}
p_grid[np.argmin(loss)]
```

## 3.3 Sampling to simulate prediction

### 3.3.1 Dummy data
```{python}
stats.binom(n=2, p=0.7).pmf(k=np.arange(0, 4))
```

```{python}
stats.binom(n=2, p=0.7).pmf(k=1)
```

```{python}
stats.binom(n=2, p=0.7).pmf(k=10)
```

```{python}
dummy_w = stats.binom(n=2, p=0.7).rvs(size=100_000)
valmap(lambda x: x / len(dummy_w), frequencies(dummy_w))
```

```{python}
dummy_w = stats.binom(n=9, p=0.7).rvs(size=100_000)
fig, ax = plt.subplots()

ax.hist(dummy_w)
```

### 3.3.2 Model checking

#### 3.3.2.2 Is the model adequate?
```{python}
w = stats.binom(n=9, p=0.6).rvs(size=10_000)
```

```{python}
w = stats.binom(n=9, p=samples).rvs()
```

## 3.4 Summary

## 3.5 Practice

```{python}
p_grid = np.linspace(start=0, stop=1, num=1000)
prior = np.repeat(1, 1000)
likelihood = stats.binom(n=9, p=p_grid).pmf(k=6)
posterior = prior * likelihood
posterior = posterior / np.sum(posterior)
```

```{python}
samples = rng.choice(a=p_grid, size=10_000, replace=True, p=posterior)
```

### 3E1
How much posterior probability lies below $p = 0.2$?

```{python}
np.mean(samples < 0.2)
```

### 3E2
How much posterior probability lies below $p = 0.8$?
```{python}

```