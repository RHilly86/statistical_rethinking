---
title: "Chapter 3 Notes"
format: html
---

```{r}
library(rethinking)
library(collapse)
library(distributions3)
library(ggplot2)

set_collapse(mask = "manip")
```

```{r}
Pr_Positive_Vampire = 0.95
Pr_Positive_Mortal = 0.01
Pr_Vampire = 0.001
Pr_Positive = Pr_Positive_Vampire * Pr_Vampire + Pr_Positive_Mortal * (1 - Pr_Vampire)
Pr_Vampire_Positive = Pr_Positive_Vampire * Pr_Vampire / Pr_Positive

Pr_Vampire_Positive
```

## 3.1 Sampling from a grid-approximate posterior

```{r}
p_grid = seq(0, 1, length.out = 1000)
prob_p = rep(1, 1000)
prob_data = pdf(Binomial(size = 9, p = p_grid), 6)
posterior = prob_data * prob_p
posterior = posterior / sum(posterior)
```

We can use our posterior distribution to sample values of $p$. Here, each value of $p$ will appear in proportion to the posterior plausibility of each value.

```{r}
samples = sample(p_grid, size = 10000, replace = TRUE, prob = posterior)
```

```{r}
plot(samples)
```

```{r}
hist(samples)
```

## 3.2 Sampling to summarize

We can use our posterior distribution to answer questions like:

* How much posterior probability lies below a parameter value?
* How much posterior probability lies between two parameter values?
* Which parameter value marks the lower 5% of the posterior probability?
* Which range of parameter values contains 90% of the posterior probability?
* Which parameter value has the highest posterior probability?

### 3.2.1 Intervals of defined boundaries

We won't be able to do below since grid approximation isn't usually practical (especially with multiple parameters). However, we can use the samples we pulled above and do the same thing.

```{r}
sum(posterior[p_grid < 0.5])
```

Here, we do what we did above but using samples of $p$:

16.81% posterior probability lies below $p = 0.5$.

```{r}
sum(samples < 0.5) / 1e4
mean(samples < 0.5)
```

61.52% posterior probability lies between $p = 0.5 \text{ and } p = 0.75$

```{r}
sum(samples > 0.5 & samples < 0.75) / 1e4
mean(samples > 0.5 & samples < 0.75)
```

### 3.2.2 Intervals of defined mass

We can also get how much posterior probability is between different hypothetical values of $p$.

For instance, we see that 80% of the posterior probability lies between 0 and 0.75:

```{r}
quantile(samples, 0.8)
```

While the middle 80% is between 0.445 and 0.812

```{r}
quantile(samples, c(0.1, 0.9))
```

However, this method of finding the posterior probability between two values of $p$ is not as accurate when the distribution is asymmetric. 

```{r}
p_grid = seq(0, 1, length.out = 1000)
prior = rep(1, 1000)
likelihood = pdf(Binomial(size = 9, p = p_grid), 6)
posterior = likelihood * prior
posterior = posterior / sum(posterior)
samples = sample(p_grid, size = 10000, replace = TRUE, prob = posterior)
```

In those cases, we can use the **Highest Density Interval (HDI)**, which is the narrowest interval containing a certain amount of the posterior probability.

```{r}
PI(samples, prob = 0.5)
```

```{r}
HPDI(samples, prob = 0.5)
```

```{r}
p_grid[which.max(posterior)]
```

```{r}
mean(samples)
median(samples)
```

```{r}
sum(posterior * abs(0.5 - p_grid))
```

```{r}
loss = sapply(p_grid, \(d) sum(posterior * abs(d - p_grid)))
```

```{r}
p_grid[which.min(loss)]
```

## 3.3 Sampling to simulate prediction

### 3.3.1 Dummy data

```{r}
pdf(Binomial(size = 2, p = 0.7), 0:3)
```

```{r}
random(Binomial(size = 2, p = 0.7), 1)
```

```{r}
random(Binomial(size = 2, p = 0.7), 10)
```

```{r}
dummy_w = random(Binomial(size = 2, p = 0.7), 1e5)
table(dummy_w)
```

```{r}
dummy_w = random(Binomial(size = 9, p = 0.7), 1e5)
simplehist(dummy_w, xlab = "dummy water count")
```

### 3.3.2 Model checking

#### 3.3.2.2 Is the model adequate?

```{r}
w = random(Binomial(size = 9, p = 0.6), 1e4)
```

```{r}
w = random(Binomial(size = 9, p = samples))
```


## 3.4 Summary

## 3.5 Practice

```{r}
p_grid = seq(0, 1, length.out = 1000)
prior = rep(1, 1000)
likelihood = pdf(Binomial(size = 9, p = p_grid), 6)
posterior = likelihood * prior
posterior = posterior / sum(posterior)
```

```{r}
set.seed(100)
samples = sample(p_grid, size = 1e4, prob = posterior, replace = TRUE)
```

### 3E1
How much posterior probability lies below $p = 0.2$?

```{r}
mean(samples < 0.2)
```

### 3E2
How much posterior probability lies below $p = 0.8$?

```{r}
mean(samples < 0.8)
```

### 3E3
How much posterior probability lies between $p = 0.2$ and $p = 0.8$?

```{r}
mean(samples > 0.2 & samples < 0.8)
```

### 3E4
20% of the posterior probability lies below which value of $p$?
```{r}
quantile(samples, 0.2)
```

### 3E5
20% of the posterior probability lies above which value of $p$?

```{r}
quantile(samples, 0.8)
```

### 3E6
Which values of $p$ contain the narrowest interval equal to 66% of the posterior probability?

```{r}
HPDI(samples, prob = 0.66)
```

### 3E7
Which values of $p$ contain 66% of the posterior probability, assuming equal posterior probability both below and above the interval?

```{r}
PI(samples, prob = 0.66)
```

### 3M1
Suppose the globe tossing data had turned out to be 8 water in 15 tosses. Construct the posterior distribution, using grid approximation. Use the same flat prior as before.

```{r}
p_grid = seq(0, 1, length.out = 1000)
prior = rep(1, 1000)
likelihood = pdf(Binomial(size = 15, p = p_grid), 8)
posterior = prior * likelihood
posterior = posterior / sum(posterior)
```

### 3M2
Draw 10,000 samples from the grid approximation from above. Then use the samples to calculate the 90% HPDI for $p$.

```{r}
samples = sample(p_grid, size = 1e4, prob = posterior, replace = TRUE)
HPDI(samples, prob = 0.9)
```

### 3M3
Construct a posterior predictive check for this model and data. This means simulate the distribution of samples, averaging over the posterior uncertainty in $p$. What is the probability of observing 8 water in 15 tosses?

```{r}
w = random(Binomial(size = 15, p = samples))

mean(w == 8)
table(w) / length(w)
```

### 3M4
Using the posterior distribution constructed from the new (8/15) data, now calculate the probability of observing 6 water in 9 tosses.

```{r}
prior = posterior
likelihood = pdf(Binomial(size = 9, p = p_grid), 6)
posterior = prior * likelihood
posterior = posterior / sum(posterior)
```

```{r}
samples = sample(p_grid, size = 1e4, prob = posterior, replace = TRUE)
w = random(Binomial(size = 9, p = samples))

mean(w == 6)
table(w) / length(w)
```

### 3M5
Start over at 3M1, but now use a prior that is zero below $p = 0.5$ and a constant above $p = 0.5$. This corresponds to prior information that a majority of the Earth's surface is water. Repeat each problem above and compare the inferences. What difference does the better prior make? If it helps, compare inferences (using both priors) to the true value $p = 0.7$.

```{r}
p_grid = seq(0, 1, length.out = 1000)
prior = ifelse(p_grid < 0.5, 0, 1)
likelihood = pdf(Binomial(size = 15, p = p_grid), 8)
posterior = prior * likelihood
posterior = posterior / sum(posterior)
```

```{r}
samples = sample(p_grid, size = 1e4, prob = posterior, replace = TRUE)
HPDI(samples, prob = 0.9)
```

```{r}
w = random(Binomial(size = 15, p = samples))

mean(w == 8)
table(w) / length(w)
```

```{r}
prior = posterior
likelihood = pdf(Binomial(size = 9, p = p_grid), 6)
posterior = prior * likelihood
posterior = posterior / sum(posterior)
```

```{r}
samples = sample(p_grid, size = 1e4, prob = posterior, replace = TRUE)
HPDI(samples, prob = 0.9)
```

```{r}
w = random(Binomial(size = 9, p = samples))

mean(w == 6)
table(w) / length(w)
```

### 3M6
Suppose you want to estimate the Earth's proportion of water very precisely. Specifically, you want the 99% percentile interval of the posterior distribution of $p$ to be only 0.05 wide. This means the distance between the upper and lower bound of the interval should be 0.05. How many times will you have to toss the globe to do this?
```{r}
p = Binomial(size = 1, p = 0.7)
pi-=  _width = numeric()
tosses = 0

while(pi_width != 0.5) {
    outcome = random(p)
}
```

## Hard

### 3H1

### 3H2

### 3H3

### 3H4

### 3H5